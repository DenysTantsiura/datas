{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Частина 1__\n",
    "\n",
    "В якості домашнього завдання вам пропонується створити нейронну мережу за допомогою механізмів `Keras`, яка буде класифікувати товари із датасету `fasion_mnist`.\n",
    "\n",
    "На відміну від попереднього завдання вам пропонується створити `згорткову нейромережу`. Підберіть архітектуру мережі та навчіть її на даних із датасету fasion_mnist. Спробуйте досягти `максимально можливої точності класифікації` за рахунок `маніпуляції параметрами` мережі. `Порівняйте точність отриманої згорткової мережі з точністю багатошарової мережі з попереднього завдання`. Зробіть висновки.\n",
    "<br><br>\n",
    "__Частина 2__\n",
    "\n",
    "В цій частині ми знову будемо працювати з датасетом `fasion_mnist`.\n",
    "\n",
    "На відміну від попереднього завдання вам пропонується створити `згорткову нейромережу`, що використовує `VGG16` в якості згорткової основи.\n",
    "\n",
    "Навчіть отриману мережу на даних із датасету fasion_mnist. Спробуйте досягти `максимально можливої точності класифікації` за рахунок `маніпуляції параметрами` мережі. Під час навчання використовуйте `прийоми донавчання` та `виділення ознак`.\n",
    "\n",
    "`Порівняйте точність отриманої згорткової мережі з точністю багатошарової мережі з попереднього завдання`. Зробіть висновки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.activations import mish, relu, sigmoid, softmax\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import layers\n",
    "from keras import losses\n",
    "from keras import metrics\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Part 1__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Dataset__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://www.tensorflow.org/api_docs/python/tf/keras/datasets/fashion_mnist/load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dtype('uint8'), (28, 28))"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(images_train, labels_train), (images_test, labels_test) = fashion_mnist.load_data()\n",
    "images_train.dtype, images_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# each sample (28x28) = 784\n",
    "NUM_FEATURES = images_train[0].shape[0] * images_train[0].shape[1]\n",
    "NUM_FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "255.0"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_train, images_test = np.array(images_train, np.float32), np.array(images_test, np.float32)  # .astype('float32')\n",
    "\n",
    "# images_train, images_test = images_train.reshape([-1, NUM_FEATURES]), images_test.reshape([-1, NUM_FEATURES])\n",
    "print(min(images_train.min(), images_test.min()))\n",
    "maximum = max(images_train.max(), images_test.max()) # np.max([images_train.max(), images_test.max()]) \n",
    "maximum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize (min = 0)\n",
    "images_train, images_test = images_train / maximum, images_test / maximum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (10000, 28, 28), (60000,), (10000,))"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_train.shape, images_test.shape, labels_train.shape, labels_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 0)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(labels_train.max(), labels_test.max()), min(labels_train.min(), labels_test.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_CLASSES = len(set(np.concatenate((labels_train, labels_test), axis=0)))\n",
    "NUM_CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_valid, images_test, labels_valid, labels_test = train_test_split(\n",
    "                                                                        images_test, \n",
    "                                                                        labels_test, \n",
    "                                                                        test_size=0.5, \n",
    "                                                                        shuffle=True, \n",
    "                                                                        stratify=labels_test)  # (stratify для рівномірного розподілу за значенням y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 10), (5000, 10), (5000, 10))"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_train = tf.one_hot(labels_train, depth=NUM_CLASSES).numpy()\n",
    "labels_test = tf.one_hot(labels_test, depth=NUM_CLASSES).numpy()\n",
    "labels_valid = tf.one_hot(labels_valid, depth=NUM_CLASSES).numpy()\n",
    "# labels_train = to_categorical(labels_train, num_classes = NUM_CLASSES)\n",
    "# labels_valid = to_categorical(labels_valid, num_classes = NUM_CLASSES)\n",
    "labels_train.shape, labels_test.shape, labels_valid.shape  # ((60000, 10), (10000, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28),\n",
       " (5000, 28, 28),\n",
       " (5000, 28, 28),\n",
       " (60000, 10),\n",
       " (5000, 10),\n",
       " (5000, 10))"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_train.shape, images_test.shape, images_valid.shape, labels_train.shape, labels_test.shape, labels_valid.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Model__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://keras.io/api/layers/convolution_layers/convolution2d/\n",
    "- https://keras.io/api/layers/pooling_layers/max_pooling2d/\n",
    "- https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_window_size_1 = 3  # 5, 9, 11\n",
    "kernel_window_size_2 = 3  # 5, 9, 11\n",
    "kernel_window_size_3 = 3  # 5, 9, 11\n",
    "filters_1 = 32  # output_depth\n",
    "filters_2 = 64\n",
    "filters_3 = 64\n",
    "filters_4 = 64\n",
    "# activation_functions = ['relu' for _ in range(4)] + ['softmax']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential([\n",
    "                           layers.Conv2D(\n",
    "                                         filters_1, \n",
    "                                         (kernel_window_size_1, kernel_window_size_1), \n",
    "                                         activation='relu', \n",
    "                                         input_shape=(images_train.shape[1], images_train.shape[2], 1)  # channels 1 - gray, 3 - RGB\n",
    "                                         ),  \n",
    "                           layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "                           layers.Conv2D(\n",
    "                                         filters_2, \n",
    "                                         (kernel_window_size_2, kernel_window_size_2), \n",
    "                                         activation='relu'\n",
    "                                         ),\n",
    "                           layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "                           layers.Conv2D(\n",
    "                                         filters_3, \n",
    "                                         (kernel_window_size_3, kernel_window_size_3), \n",
    "                                         activation='relu'\n",
    "                                         ),\n",
    "\n",
    "                           layers.Flatten(),\n",
    "                           layers.Dense(\n",
    "                                        filters_4, \n",
    "                                        activation='relu'\n",
    "                                        ),\n",
    "\n",
    "                           layers.Dense(\n",
    "                                        NUM_CLASSES, \n",
    "                                        activation='softmax'\n",
    "                                        )\n",
    "                           ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_12 (Conv2D)          (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPoolin  (None, 13, 13, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 11, 11, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_9 (MaxPoolin  (None, 5, 5, 64)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_14 (Conv2D)          (None, 3, 3, 64)          36928     \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 576)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 64)                36928     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 93322 (364.54 KB)\n",
      "Trainable params: 93322 (364.54 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "              optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy']\n",
    "              )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Training__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "938/938 [==============================] - 9s 9ms/step - loss: 0.5649 - accuracy: 0.7914\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 9s 10ms/step - loss: 0.3370 - accuracy: 0.8785\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 9s 10ms/step - loss: 0.2838 - accuracy: 0.8967\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 10s 10ms/step - loss: 0.2527 - accuracy: 0.9071\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 10s 10ms/step - loss: 0.2282 - accuracy: 0.9164\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(images_train, labels_train, epochs=5, batch_size=64)  # w/o valid data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Error Analysis__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.5649124383926392,\n",
       "  0.33701926469802856,\n",
       "  0.28380846977233887,\n",
       "  0.2527290880680084,\n",
       "  0.22820459306240082],\n",
       " 'accuracy': [0.7913500070571899,\n",
       "  0.8785333037376404,\n",
       "  0.8966666460037231,\n",
       "  0.9071000218391418,\n",
       "  0.9164333343505859]}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 1s 3ms/step - loss: 0.2791 - accuracy: 0.9038\n",
      "0.9038000106811523\n",
      "157/157 [==============================] - 0s 3ms/step - loss: 0.2779 - accuracy: 0.9006\n",
      "0.900600016117096\n"
     ]
    }
   ],
   "source": [
    "valid_loss, valid_acc = model.evaluate(images_valid, labels_valid)\n",
    "print(valid_acc)\n",
    "test_loss, test_acc = model.evaluate(images_test, labels_test)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_2twin_2d(y00: np.array, y01: np.array, y10: np.array, y11: np.array, suptitle: str) -> None:\n",
    "    x = range(1, len(y00) + 1)\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 8))\n",
    "\n",
    "    axs[0].plot(x, y00, 'bo', label='Training loss')\n",
    "    axs[0].plot(x, y01, 'g', label='Validation loss')\n",
    "    axs[0].title.set_text('Training and validation loss')\n",
    "    axs[0].set_xlabel('Epochs')\n",
    "    axs[0].set_ylabel('Loss')\n",
    "    axs[0].legend()\n",
    "    axs[0].grid()\n",
    "\n",
    "    axs[1].plot(x, y10, 'bo', label='Training acc')\n",
    "    axs[1].plot(x, y11, 'g', label='Validation acc')\n",
    "    axs[1].title.set_text('Training and validation acc')\n",
    "    axs[1].set_xlabel('Epochs')\n",
    "    axs[1].set_ylabel('Acc')\n",
    "    axs[1].legend()\n",
    "    axs[1].grid()\n",
    "    \n",
    "    fig.suptitle(suptitle)\n",
    "    # plt.legend()\n",
    "    # plt.grid()\n",
    "    axs[1].grid()\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __repeating+__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_NN(n_hidden_: list, activation: list, optimizer=optimizers.SGD, learning_rate=0.1) -> Sequential:\n",
    "    model = Sequential(name=f'''m-{''.join(map(str, [f'{i}-' for i in n_hidden_[1:-1]]))}{''.join(map(str, [f'{a}-' for a in activation[:-1]]))}{optimizer.__name__}-{learning_rate}''')\n",
    "    n_layers = len(n_hidden_)\n",
    "    for layer in range(1, n_layers):\n",
    "        model.add(Dense(n_hidden_[layer], activation=activation[layer-1], input_shape=(n_hidden_[layer-1],)))\n",
    "\n",
    "    model.compile(\n",
    "                  optimizer=optimizer(learning_rate=learning_rate), # legacy\n",
    "                  loss=losses.CategoricalCrossentropy(),  # 'categorical_crossentropy' losses.binary_crossentropy,\n",
    "                  metrics=['accuracy', metrics.CategoricalAccuracy()]  # metrics.BinaryAccuracy()  CategoricalAccuracy()\n",
    "                  )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []  # 9216 models # !!! more then 32 GB RAM!!!!, ... try 1260 = 28sec\n",
    "for set_layers in n_hiddens_:  # 584\n",
    "    for act in bundle_activations:  # 39\n",
    "        if len(set_layers) == (len(act) + 1):  # 1024  -> 210\n",
    "            for opt in optimizators:  # 3\n",
    "                for lr in learning_rates:  # 2\n",
    "                    models.append(create_NN(n_hidden_=set_layers, activation=act, optimizer=opt, learning_rate=lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retest_NN(set_layers: list, act: list, opt: optimizers, lr: float, bs: int, epochs: int=100) -> None:\n",
    "    \"\"\"For retest one model.\"\"\"\n",
    "    models = [create_NN(\n",
    "                        n_hidden_=set_layers, \n",
    "                        activation=act, \n",
    "                        optimizer=opt, \n",
    "                        learning_rate=lr\n",
    "                        )] \n",
    "\n",
    "    history = {}\n",
    "    history[f'{models[0].name}({bs})'] = models[0].fit(\n",
    "                                                       x_train,\n",
    "                                                       y_true,\n",
    "                                                       epochs=epochs,\n",
    "                                                       batch_size=bs,\n",
    "                                                       validation_data=(x_test, y_truev)\n",
    "                                                       )\n",
    "\n",
    "    for model in history:\n",
    "        history_dict = history[model].history\n",
    "\n",
    "        loss_values = history_dict['loss']\n",
    "        val_loss_values = history_dict['val_loss']\n",
    "        acc_values = history_dict['categorical_accuracy']\n",
    "        val_acc_values = history_dict['val_categorical_accuracy']   \n",
    "\n",
    "        draw_2twin_2d(loss_values, val_loss_values, acc_values, val_acc_values, suptitle=model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Сonclusions 1__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Part 2__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Dataset__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Model__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Training__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Error Analysis__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Сonclusions 2__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datas-cYPLqW4U-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
