{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Власний обчислювальний граф__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework.ops import EagerTensor\n",
    "\n",
    "\n",
    "def formula(x: EagerTensor, y: EagerTensor, b: EagerTensor) -> EagerTensor:\n",
    "    \"\"\"Some operations with tensors.\"\"\"\n",
    "    x = tf.matmul(x, y)\n",
    "    x = x + b\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "function_that_uses_a_graph = tf.function(formula)  # приймає у якості аргумента звичайну функцію і генерує обчислювальний граф.\n",
    "# tf.function можна використовувати як декоратор\n",
    "\n",
    "x1 = tf.constant([[1.0, 2.0]])  # створюємо матрицю як тензор\n",
    "y1 = tf.constant([[2.0], [3.0]])  # створюємо матрицю як тензор\n",
    "b1 = tf.constant(4.0)  # створюємо скаляр як тензор\n",
    "\n",
    "z1 = tf.convert_to_tensor(np.array([1, 2, 3]))  # перетворюємо масив на тензор\n",
    "z1 = z1.numpy()  # зворотньо перетворюємо тензор на масив\n",
    "\n",
    "\n",
    "orig_value = formula(x1, y1, b1).numpy()\n",
    "tf_function_value = function_that_uses_a_graph(x1, y1, b1).numpy()\n",
    "\n",
    "assert(orig_value == tf_function_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "oops!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39massert\u001b[39;00m orig_value \u001b[39m!=\u001b[39m tf_function_value, \u001b[39m'\u001b[39m\u001b[39moops!\u001b[39m\u001b[39m'\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: oops!"
     ]
    }
   ],
   "source": [
    "assert orig_value != tf_function_value, 'oops!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.function можна використовувати як декоратор\n",
    "@tf.function\n",
    "def function_that_uses_a_graph(x: EagerTensor, y: EagerTensor, b: EagerTensor) -> EagerTensor:\n",
    "    \"\"\"Some operations with tensors.\"\"\"\n",
    "    x = tf.matmul(x, y)\n",
    "    x = x + b\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "tf_function_value = function_that_uses_a_graph(x1, y1, b1).numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У нейронних мережах часто використовується активаційна функція ReLU:<br>\n",
    "- `f(x) = max(x, 0)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def relu_activation(x):\n",
    "    if tf.greater(x, 0):\n",
    "        return x\n",
    "    \n",
    "    return 0\n",
    "\n",
    "\n",
    "print(relu_activation(tf.constant(1)).numpy())\n",
    "print(relu_activation(tf.constant(-1)).numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Автоматичне диференціювання__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 1 / x ** 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тепер похідну можна обчислити так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(-0.25, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "x = tf.Variable(2.0)\n",
    "\"\"\"\n",
    "За допомогою класу GradientTape tensorflow записує всі операції,\n",
    "необхідні для обчислення похідної в змінну tape . Для того, щоб обчислити\n",
    "значення похідної в конкретній точці, потрібно викликати метод gradient\n",
    "\"\"\"\n",
    "with tf.GradientTape() as tape:\n",
    "    y = f(x)\n",
    "    dydx = tape.gradient(y, x)\n",
    "    print(dydx)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Створення нейронної мережі__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, щоб створити шар нейромережі, достатньо написати клас, який успадковується від `tf.Module`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Нехай потрібно написати\n",
    "примітивну нейронну мережу, яка обчислює значення виразу \n",
    "w*x + b\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class SimpleModule(tf.Module):\n",
    "\n",
    "    def __init__(self, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.w = tf.Variable(5.0)\n",
    "        self.b = tf.Variable(5.0)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"Обов'язковою є реалізація цтого магічного методу, \n",
    "        оскільки всі фактичні обчислення відбуваються саме тут.\"\"\"\n",
    "        return self.w * x + self.b\n",
    "    \n",
    "simple_module = SimpleModule(name='simple')\n",
    "simple_module(tf.constant(5.0)).numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тепер ми можемо спробувати створити складнішу нейронну мережу.\n",
    "Нехай потрібно створити нейромережу, в якій 2 шари. На першому шарі\n",
    "має 3 нейрони з трьома входами, на другому - 1, в якості активаційної\n",
    "функції використовуватимемо ReLU. Реалізація такої нейромережі\n",
    "представлена далі."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: tf.Tensor([[0.]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "class DenseLayer(tf.Module):\n",
    "    \"\"\"Цей клас є універсальним, оскільки за рахунок параметрів in_features та\n",
    "    out_features ми можемо легко налаштовувати кількість входів та виходів\n",
    "    шару.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.w = tf.Variable(\n",
    "                             tf.random.normal([in_features, out_features]), \n",
    "                             name='w'\n",
    "                             )\n",
    "        self.b = tf.Variable(tf.zeros([out_features]), name='b')\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        y = tf.matmul(x, self.w) + self.b\n",
    "\n",
    "        return tf.nn.relu(y)\n",
    "\n",
    "\n",
    "class NN(tf.Module):\n",
    "    \"\"\"NN (скорочення від Neural Network), який інкапсулює\n",
    "    необхідні два шари і в якому визначено перетворення в магічному методі\n",
    "    def __call__(self, x) . Таким чином, можна створювати нейронні\n",
    "    мережі майже будь-якої складності.\n",
    "    \"\"\"\n",
    "    def __init__(self, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.layer_1 = DenseLayer(in_features=3, out_features=3)\n",
    "        self.layer_2 = DenseLayer(in_features=3, out_features=1)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = self.layer_1(x)\n",
    "\n",
    "        return self.layer_2(x)\n",
    "    \n",
    "\n",
    "nn = NN(name='neural_network')\n",
    "print('Results:', nn(tf.constant([[2.0, 2.0, 2.0]])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Навчання нейронної мережі__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "маємо лінійну модель виду:<br>\n",
    "a(x) = ω1 * x1 + ω2 * x2 + ... + ωn * xn + b<br>\n",
    "Створимо клас для даної моделі."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class LinearModel(tf.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.w = tf.Variable(5.0)\n",
    "        self.b = tf.Variable(0.0)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return self.w * x + self.b "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тепер визначимо дві функції: `loss` та `train`. <br>Одна буде обчислювати\n",
    "помилку, а друга - підлаштовувати ваги."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(target_y, predicted_y):\n",
    "    return tf.reduce_mean(tf.square(target_y - predicted_y))\n",
    "\n",
    "def train(model, x, y, learning_rate):\n",
    "    with tf.GradientTape() as t:\n",
    "        current_loss = loss(y, model(x))\n",
    "         # використовуємо механізми tensorflow для автоматичного диференціювання:\n",
    "        dw, db = t.gradient(current_loss, [model.w, model.b]) \n",
    "        model.w.assign_sub(learning_rate * dw) \n",
    "        # Для оновлення ваг ми використовуємо метод assign_sub , який веде себе як оператор -=\n",
    "        model.b.assign_sub(learning_rate * db)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "знадобиться ще одна функція, у якій ми\n",
    "будемо безпосередньо тренувати модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, x, y):\n",
    "    for epoch in range(10):\n",
    "        train(model, x, y, learning_rate=0.1)\n",
    "        current_loss = loss(y, model(x))\n",
    "        print(f'loss: {current_loss}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "щоб протестувати навчання моделі згенеруємо тестові дані"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRUE_W = 3.0\n",
    "TRUE_B = 2.0\n",
    "NUM_EXAMPLES = 1000\n",
    "x = tf.random.normal(shape=[NUM_EXAMPLES])\n",
    "noise = tf.random.normal(shape=[NUM_EXAMPLES])\n",
    "y = x * TRUE_W + TRUE_B + noise"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Збірка та Запуск__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 6.386050224304199\n",
      "loss: 4.425792217254639\n",
      "loss: 3.1793289184570312\n",
      "loss: 2.3867335319519043\n",
      "loss: 1.882735013961792\n",
      "loss: 1.562245488166809\n",
      "loss: 1.3584457635879517\n",
      "loss: 1.2288470268249512\n",
      "loss: 1.146432638168335\n",
      "loss: 1.0940228700637817\n"
     ]
    }
   ],
   "source": [
    "linear_model = LinearModel()\n",
    "training_loop(linear_model, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.LinearModel at 0x7fb60027dbd0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'linear_model'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_model.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8337007"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_model.b.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.170131"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_model.w.numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datas-cYPLqW4U-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
