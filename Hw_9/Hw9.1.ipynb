{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В якості домашнього завдання вам пропонується створити нейронну мережу за допомогою механізмів Keras, яка буде __класифікувати__ товари із датасету `fasion_mnist`.\n",
    "\n",
    "Вам належить запропонувати свою власну архітектуру мережі. Точність найнаївнішої, але адекватної нейромережі становить приблизно __91%__. Точність вашої моделі повинна бути `не нижчою` за цей показник. Щоб досягти таких значень вам знадобиться `поекспериментувати з гіперпараметрами мережі`:\n",
    "\n",
    "- кількість шарів;\n",
    "\n",
    "- кількість нейронів;\n",
    "\n",
    "- функції активації;\n",
    "\n",
    "- кількість епох;\n",
    "\n",
    "- розмір батчу;\n",
    "\n",
    "- вибір оптимізатора;\n",
    "\n",
    "- різні техніки регуляризації і т.д.\n",
    "\n",
    "Використайте вивчені `техніки виявлення проблем навчання` нейронної мережі, і потім `поекспериментуйте`.\n",
    "\n",
    "Рішення оформіть у вигляді окремого ноутбука."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from typing import Optional, Union\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.activations import mish, relu, sigmoid, softmax\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import losses\n",
    "from keras import metrics\n",
    "from keras import optimizers\n",
    "\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.activations import sigmoid\n",
    "# from tensorflow.keras.datasets import mnist\n",
    "# from tensorflow.keras.metrics import Accuracy\n",
    "# from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### __Dataset__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning_rate = 0.001  # скорость обучения нейронной сети\n",
    "# batch_size = 256  # пересчитывать веса сети мы будем не на всей выборке, а на ее случайном подможестве из batch_size элементов\n",
    "\n",
    "# n_hidden_1 = 128  # количество нейронов 1-го слоя\n",
    "# n_hidden_2 = 256  # количество нейронов 2-го слоя\n",
    "\n",
    "# BUFFER_SIZE = 5000  # This dataset fills a buffer with buffer_size elements, then randomly samples elements from this buffer, replacing the selected elements with new elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dtype('uint8'), (28, 28))"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "x_train.dtype, x_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FEATURES = x_train[0].shape[0] * x_train[0].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "255.0"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_test = np.array(x_train, np.float32), np.array(x_test, np.float32)\n",
    "\n",
    "# each sample (28x28) to 1x784\n",
    "x_train, x_test = x_train.reshape([-1, NUM_FEATURES]), x_test.reshape([-1, NUM_FEATURES])\n",
    "print(min(x_train.min(), x_test.min()))\n",
    "maximum = max(x_train.max(), x_test.max()) # np.max([x_train.max(), x_test.max()]) \n",
    "maximum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize (min = 0)\n",
    "x_train, x_test = x_train / maximum, x_test / maximum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 784), (10000, 784), (60000,), (10000,))"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 0)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(y_train.max(), y_test.max()), min(y_train.min(), y_test.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_CLASSES = len(set(np.concatenate((y_train, y_test), axis=0)))\n",
    "NUM_CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 10), (10000, 10))"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = tf.one_hot(y_train, depth=NUM_CLASSES).numpy()\n",
    "y_truev = tf.one_hot(y_test, depth=NUM_CLASSES).numpy()\n",
    "y_true.shape, y_truev.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### __Model__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://www.tensorflow.org/api_docs/python/tf/keras/optimizers\n",
    "- https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of hidden layers\n",
    "n_layers = [el for el in range(1, 4)]  # 1, 2, 3\n",
    "n_of_neurons = [2**el for el in range(1, 9)] # [[2**el for el in range(1, 9)] for _ in n_layers]  # 8\n",
    "\n",
    "activations = ['mish', 'relu', 'sigmoid', 'softmax']\n",
    "\n",
    "optimizators = [optimizers.legacy.SGD, optimizers.legacy.RMSprop, optimizers.legacy.Adam]  # 3\n",
    "learning_rates = [10**el for el in range(-3, 0)]  # 3\n",
    "n_epoch = 100  # max limit\n",
    "batch_sizes = [2**el for el in range(7, 10)]  # 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hiddens_ = []\n",
    "bundle_activations  = []\n",
    "for s in n_layers:\n",
    "    for el in list(itertools.product(n_of_neurons, repeat=s)):\n",
    "        n_hidden_i = []\n",
    "        n_hidden_i.append(NUM_FEATURES)\n",
    "        n_hidden_i.extend(el)\n",
    "        n_hidden_i.append(NUM_CLASSES)\n",
    "\n",
    "        n_hiddens_.append(n_hidden_i)\n",
    "\n",
    "    for el in list(itertools.product(activations[:-1], repeat=s)):\n",
    "        activat = []\n",
    "        activat.extend(el)\n",
    "        activat.append(activations[-1])\n",
    "\n",
    "        bundle_activations.append(activat)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(584, 39)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(n_hiddens_), len(bundle_activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cx = 0\n",
    "for set_layers in n_hiddens_:  # 584\n",
    "    for act in activations:  # 39\n",
    "        if len(set_layers) == (len(act) + 1):\n",
    "            cx+=1\n",
    "\n",
    "cx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_NN(n_hidden_: list, activation: list, optimizer=optimizers.SGD, learning_rate=0.1) -> Sequential:\n",
    "    model = Sequential(name=f'''m-{[f'{i}-' for i in n_hidden_[1:-1]]}{[f'{a}-' for a in activation[:-1]]}{optimizer.__name__}-{learning_rate}''')\n",
    "    n_layers = len(n_hidden_)\n",
    "    for layer in range(1, n_layers):\n",
    "        model.add(Dense(n_hidden_[layer], activation=activation[layer-1], input_shape=(n_hidden_[layer-1],)))\n",
    "\n",
    "    model.compile(\n",
    "                  optimizer=optimizer(learning_rate=learning_rate), # legacy\n",
    "                  loss=losses.CategoricalCrossentropy(),  # 'categorical_crossentropy' losses.binary_crossentropy,\n",
    "                  metrics=['accuracy']  # metrics.BinaryAccuracy()\n",
    "                  )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []  # 9216  # !!! more then 32 GB RAM!!!!\n",
    "for set_layers in n_hiddens_:  # 584\n",
    "    for act in bundle_activations:  # 39\n",
    "        if len(set_layers) == (len(act) + 1):  # 1024\n",
    "            for opt in optimizators:  # 3\n",
    "                for lr in learning_rates:  # 3\n",
    "                    models.append(create_NN(n_hidden_=set_layers, activation=act, optimizer=opt, learning_rate=lr))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### __Training__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {}  # 27648 = 38+ days training\n",
    "for model in models:\n",
    "    for bs in batch_sizes:  # 3\n",
    "        history[f'{model.name}({bs})'] = model.fit(\n",
    "                                                    x_train,\n",
    "                                                    y_true,\n",
    "                                                    epochs=100,\n",
    "                                                    batch_size=bs,\n",
    "                                                    validation_data=(x_test, y_truev)\n",
    "                                                    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### __Error Analysis__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' \\nloss - список, в якому зберігається історія змін помилки на навчальних даних залежно від епохи\\nval_loss - список, де зберігається історія змін помилки на тестових даних залежно від епохи\\naccuracy - список, в якому зберігається історія змін точності на навчальних даних залежно від епохи\\nval_accuracy - список, де зберігається історія змін точності на тестових даних залежно від епохи\\n'"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(history[0].history.keys())\n",
    "\"\"\" \n",
    "loss - список, в якому зберігається історія змін помилки на навчальних даних залежно від епохи\n",
    "val_loss - список, де зберігається історія змін помилки на тестових даних залежно від епохи\n",
    "accuracy - список, в якому зберігається історія змін точності на навчальних даних залежно від епохи\n",
    "val_accuracy - список, де зберігається історія змін точності на тестових даних залежно від епохи\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_2twin_2d(y00: np.array, y01: np.array, y10: np.array, y11: np.array, suptitle: str) -> None:\n",
    "    x = range(1, len(y00) + 1)\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 8))\n",
    "\n",
    "    axs[0].plot(x, y00, 'bo', label='Training loss')\n",
    "    axs[0].plot(x, y01, 'g', label='Validation loss')\n",
    "    axs[0].title.set_text('Training and validation loss')\n",
    "    axs[0].set_xlabel('Epochs')\n",
    "    axs[0].set_ylabel('Loss')\n",
    "    axs[0].legend()\n",
    "    axs[0].grid()\n",
    "\n",
    "    axs[1].plot(x, y10, 'bo', label='Training acc')\n",
    "    axs[1].plot(x, y11, 'g', label='Validation acc')\n",
    "    axs[1].title.set_text('Training and validation acc')\n",
    "    axs[1].set_xlabel('Epochs')\n",
    "    axs[1].set_ylabel('Acc')\n",
    "    axs[1].legend()\n",
    "    axs[1].grid()\n",
    "    \n",
    "    fig.suptitle(suptitle)\n",
    "    # plt.legend()\n",
    "    # plt.grid()\n",
    "    axs[1].grid()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in history:\n",
    "    history_dict = history[model].history\n",
    "\n",
    "    loss_values = history_dict['loss']\n",
    "    val_loss_values = history_dict['val_loss']\n",
    "    acc_values = history_dict['accuracy']\n",
    "    val_acc_values = history_dict['val_accuracy']   \n",
    "\n",
    "    draw_2twin_2d(loss_values, val_loss_values, acc_values, val_acc_values, suptitle=model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datas-cYPLqW4U-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
