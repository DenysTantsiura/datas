{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Name__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description & tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-15 10:00:32.481837: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-08-15 10:00:32.504008: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-15 10:00:32.671606: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-15 10:00:32.672809: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-15 10:00:33.460386: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import math \n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.activations import mish, relu, sigmoid, softmax\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import layers\n",
    "from keras import losses\n",
    "from keras import metrics\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!!! set randome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __`OBTAIN` & `SCRUB`__ + __`EXPLORE`__ (DATASET)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading `the prepared data:`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://www.tensorflow.org/api_docs/python/tf/keras/datasets/fashion_mnist/load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28),\n",
       " (5000, 28, 28),\n",
       " (5000, 28, 28),\n",
       " (60000, 10),\n",
       " (5000, 10),\n",
       " (5000, 10))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(images_train, labels_train), (images_test, labels_test) = fashion_mnist.load_data()\n",
    "# images_train.dtype, images_train[0].shape\n",
    "NUM_FEATURES = images_train[0].shape[0] * images_train[0].shape[1]\n",
    "# NUM_FEATURES\n",
    "images_train, images_test = np.array(images_train, np.float32), np.array(images_test, np.float32)  # .astype('float32')\n",
    "# print(min(images_train.min(), images_test.min()))  # 0  255\n",
    "maximum = max(images_train.max(), images_test.max()) # np.max([images_train.max(), images_test.max()]) \n",
    "# Normalize (min = 0):\n",
    "images_train, images_test = images_train / maximum, images_test / maximum\n",
    "# images_train.shape, images_test.shape, labels_train.shape, labels_test.shape\n",
    "max(labels_train.max(), labels_test.max()), min(labels_train.min(), labels_test.min())  # 0  9\n",
    "NUM_CLASSES = len(set(np.concatenate((labels_train, labels_test), axis=0)))  # 10\n",
    "# (10000, 10) into ((5000, 10), (5000, 10)):\n",
    "images_valid, images_test, labels_valid, labels_test = train_test_split(\n",
    "                                                                        images_test, \n",
    "                                                                        labels_test, \n",
    "                                                                        test_size=0.5,  # 50%\n",
    "                                                                        shuffle=True, \n",
    "                                                                        stratify=labels_test\n",
    "                                                                        )  # (stratify для рівномірного розподілу за значенням y)\n",
    "labels_train = tf.one_hot(labels_train, depth=NUM_CLASSES).numpy()  # 5 into [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "labels_test = tf.one_hot(labels_test, depth=NUM_CLASSES).numpy()\n",
    "labels_valid = tf.one_hot(labels_valid, depth=NUM_CLASSES).numpy()\n",
    "# labels_train.shape, labels_test.shape, labels_valid.shape  # ((60000, 10), (5000, 10), (5000, 10))\n",
    "images_train.shape, images_test.shape, images_valid.shape, labels_train.shape, labels_test.shape, labels_valid.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading by Dataset `from 'RAW-files':`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "PATH_IMGS = os.path.join(os.getcwd(), 'CIFAR-10-images-master', 'train')\n",
    "\n",
    "ids = []  # full image paths\n",
    "labels = []  # true mark 'images name-description' by label2index:\n",
    "# label2index = {kind:num for num, kind in enumerate(os.listdir(PATH_IMGS))}\n",
    "label2index = {}\n",
    "\n",
    "for num, folder in enumerate(os.listdir(PATH_IMGS)):\n",
    "    label2index[folder] = num\n",
    "    for image_name in os.listdir(os.path.join(PATH_IMGS, folder))[:500]:  # limit 500 for each - for example\n",
    "        ids.append(os.path.join(PATH_IMGS, folder, image_name))\n",
    "        labels.append(label2index[folder])\n",
    "\n",
    "NUM_CLASSES = num + 1\n",
    "NUM_CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_train, ids_valid, y_train, y_valid = train_test_split(ids, labels, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://www.tensorflow.org/api_docs/python/tf/keras/utils/Sequence\n",
    "- https://numpy.org/doc/stable/reference/random/generated/numpy.random.shuffle.html\n",
    "- https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical\n",
    "\n",
    "- https://docs.opencv.org/3.4/d4/da8/group__imgcodecs.html\n",
    "- https://www.geeksforgeeks.org/python-opencv-cv2-imread-method/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(tf.keras.utils.Sequence):\n",
    "    def __init__(self, ids: list, y: list, shuffle: bool=True, batch_size: int=256) -> None:\n",
    "        self.ids = ids  # list of imgs path\n",
    "        self.y = y\n",
    "        self.shuffle = shuffle\n",
    "        self.indexes = np.arange(len(self.ids))\n",
    "        self.batch_size = batch_size\n",
    "        self.num_features = self.num_features if self.__getitem__(0) else 0\n",
    "        \n",
    "        if shuffle:\n",
    "            self.on_epoch_end()\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.ids) // self.batch_size\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> tuple:  # get one batch\n",
    "        indexes = self.indexes[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
    "        batch_ids = [self.ids[i] for i in indexes]\n",
    "        batch_y = [tf.keras.utils.to_categorical(self.y[i], 10) for i in indexes]\n",
    "        \n",
    "        batch_X = []\n",
    "        for i in range(self.batch_size):\n",
    "            img = cv2.imread(batch_ids[i])  # i/o disk operation! but low RAM uses\n",
    "            img = np.mean(img, axis=-1)  # RGB to grayscale\n",
    "\n",
    "            batch_X.append(img.reshape(len(img[0]) * len(img)))  # into a one-dimensional vector\n",
    "        \n",
    "        self.num_features = len(img[0]) * len(img) # !!\n",
    "        \n",
    "        return np.array(batch_X), np.array(batch_y)\n",
    "        \n",
    "    def on_epoch_end(self) -> None:\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(ids_train, y_train, shuffle=True, batch_size=256)\n",
    "# train_dataset[0]\n",
    "NUM_FEATURES = train_dataset.num_features\n",
    "valid_dataset = Dataset(ids_valid, y_valid, shuffle=False, batch_size=256)\n",
    "NUM_FEATURES"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://www.tutorialkart.com/opencv/python/opencv-python-resize-image/#gsc.tab=0\n",
    "- https://stackoverflow.com/questions/64276472/valueerror-the-input-must-have-3-channels-got-input-shape-200-200-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(tf.keras.utils.Sequence):\n",
    "    def __init__(self, X, y, shuffle=True, batch_size=256, to_size=32):\n",
    "        self.X = X  # (60000, 28, 28)\n",
    "        self.y = y  # (60000, 10)\n",
    "        self.shuffle = shuffle\n",
    "        self.indexes = np.arange(len(self.X))\n",
    "        self.batch_size = batch_size\n",
    "        self.to_size = to_size\n",
    "        \n",
    "        if shuffle:\n",
    "            self.on_epoch_end()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return math.ceil(self.y.shape[0] / self.batch_size)\n",
    "    \n",
    "    def __getitem__(self, idx):  # idx = batche's numder\n",
    "        indexes = self.indexes[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
    "\n",
    "        batch_X_row = [self.X[i] for i in indexes]\n",
    "        batch_y = [self.y[i] for i in indexes]\n",
    "\n",
    "        # cv2.cvtColor(grayscale_image, cv2.COLOR_GRAY2RGB)\n",
    "        batch_X = [cv2.cvtColor(cv2.resize(img, (self.to_size, self.to_size), interpolation=cv2.INTER_NEAREST), cv2.COLOR_GRAY2RGB) for img in batch_X_row]\n",
    "\n",
    "        return np.array(batch_X), np.array(batch_y)\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading `from prepared 'RAW-files'` into `ImageDataGenerator`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kaggle competitions download -c dogs-vs-cats"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://docs.python.org/3/library/pathlib.html#pathlib.PurePath.joinpath\n",
    "- https://docs.python.org/uk/3/library/shutil.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start creation of dataset directories...\n",
      "Done preparing directories.\n",
      "Copying... into /media/denys/ftb/sprojects/datas/helps/data//train/cat/...\n",
      "Copying... into /media/denys/ftb/sprojects/datas/helps/data//validation/cat/...\n",
      "Copying... into /media/denys/ftb/sprojects/datas/helps/data//test/cat/...\n",
      "Copying... into /media/denys/ftb/sprojects/datas/helps/data//train/dog/...\n",
      "Copying... into /media/denys/ftb/sprojects/datas/helps/data//validation/dog/...\n",
      "Copying... into /media/denys/ftb/sprojects/datas/helps/data//test/dog/...\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "\n",
    "train_size = 1000  # % of num-files...\n",
    "validation_size = 500  # next\n",
    "test_size = 500  # next\n",
    "src, dst = '/media/denys/ftb/sprojects/trainCatsDogs/train/', '/media/denys/ftb/sprojects/datas/helps/data/'\n",
    "\n",
    "\n",
    "def create_dataset_directories(base_dir: str, categories: set) -> None:\n",
    "    print('Start creation of dataset directories...')\n",
    "\n",
    "    Path(base_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    directories = ['train', 'validation', 'test']\n",
    "    for directory in directories:\n",
    "        sub_dir = Path(base_dir).joinpath(directory)\n",
    "        Path(sub_dir).mkdir(parents=True, exist_ok=True)\n",
    "        [Path(sub_dir.joinpath(c)).mkdir(parents=True, exist_ok=True) for c in categories]\n",
    "\n",
    "    print('Done preparing directories.')\n",
    "\n",
    "\n",
    "def copy_data(\n",
    "              src: str, \n",
    "              dst: str, \n",
    "              example_name: str, \n",
    "              ext: str,\n",
    "              start: int, \n",
    "              end: int\n",
    "              ) -> None:\n",
    "    fnames = [f'{example_name}.{i}.{ext}' for i in range(start, end)]\n",
    "    print(f'Copying... into {dst}...')\n",
    "    for fname in fnames:\n",
    "        # print(f'Copying {fname} to {dst}')\n",
    "        shutil.copyfile(\n",
    "                        Path(src).joinpath(fname),\n",
    "                        Path(dst).joinpath(fname)\n",
    "                        )\n",
    "\n",
    "\n",
    "def identify_categories(src_dir: str) -> set:\n",
    "    all_files = [el.stem.split('.')[0] for el in Path(src_dir).iterdir() if el.is_file()]\n",
    "\n",
    "    return set(all_files)\n",
    "\n",
    "\n",
    "if Path(src).is_file() or Path(dst).is_file():\n",
    "    print('Incorrect paths, there must be a folders.')\n",
    "\n",
    "else:\n",
    "    # images in src - all in one where filenames start with label (category)\n",
    "    categories: set = identify_categories(src)\n",
    "    create_dataset_directories(dst, categories)\n",
    "\n",
    "    # if same extensions of images:\n",
    "    ext: str = {0:el.suffix[1:] for el in Path(src).iterdir() if el.is_file()}.get(0, '')\n",
    "\n",
    "    for category in categories:   # ! // \\\\ linux windows\n",
    "        copy_data(src, f'{dst}/train/{category}/', category, ext, 0, train_size)\n",
    "        copy_data(src, f'{dst}/validation/{category}/', category, ext, train_size, train_size + validation_size)\n",
    "        copy_data(src, f'{dst}/test/{category}/', category, ext, train_size + validation_size, train_size + validation_size + test_size)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### __`main.py`__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start creation of dataset directories...\n",
      "Done preparing directories.\n",
      "Copying... into /media/denys/ftb/sprojects/datas/helps/data//train/cat/...\n",
      "Copying... into /media/denys/ftb/sprojects/datas/helps/data//validation/cat/...\n",
      "Copying... into /media/denys/ftb/sprojects/datas/helps/data//test/cat/...\n",
      "Copying... into /media/denys/ftb/sprojects/datas/helps/data//train/dog/...\n",
      "Copying... into /media/denys/ftb/sprojects/datas/helps/data//validation/dog/...\n",
      "Copying... into /media/denys/ftb/sprojects/datas/helps/data//test/dog/...\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "\n",
    "def create_dataset_directories(base_dir: str, categories: set) -> None:\n",
    "    print('Start creation of dataset directories...')\n",
    "\n",
    "    Path(base_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    directories = ['train', 'validation', 'test']\n",
    "    for directory in directories:\n",
    "        sub_dir = Path(base_dir).joinpath(directory)\n",
    "        Path(sub_dir).mkdir(parents=True, exist_ok=True)\n",
    "        [Path(sub_dir.joinpath(c)).mkdir(parents=True, exist_ok=True) for c in categories]\n",
    "\n",
    "    print('Done preparing directories.')\n",
    "\n",
    "\n",
    "def copy_data(\n",
    "              src: str, \n",
    "              dst: str, \n",
    "              example_name: str, \n",
    "              ext: str,\n",
    "              start: int, \n",
    "              end: int\n",
    "              ) -> None:\n",
    "    fnames = [f'{example_name}.{i}.{ext}' for i in range(start, end)]\n",
    "    print(f'Copying... into {dst}...')\n",
    "    for fname in fnames:\n",
    "        # print(f'Copying {fname} to {dst}')\n",
    "        shutil.copyfile(\n",
    "                        Path(src).joinpath(fname),\n",
    "                        Path(dst).joinpath(fname)\n",
    "                        )\n",
    "\n",
    "\n",
    "def identify_categories(src_dir: str) -> set:\n",
    "    all_files = [el.stem.split('.')[0] for el in Path(src_dir).iterdir() if el.is_file()]\n",
    "\n",
    "    return set(all_files)\n",
    "    \n",
    "\n",
    "def main(argv=None) -> set:\n",
    "    train_size = 1000  # % of num-files...\n",
    "    validation_size = 500  # next\n",
    "    test_size = 500  # next\n",
    "\n",
    "    if argv:\n",
    "        if len(argv) != 3:\n",
    "            print('Incomplete arguments, there must be 2 paths: resource and destination.')\n",
    "            sys.exit(-1)\n",
    "\n",
    "        src, dst = argv[1], argv[2]\n",
    "\n",
    "    else:\n",
    "        src, dst = '/media/denys/ftb/sprojects/trainCatsDogs/train/', '/media/denys/ftb/sprojects/datas/helps/data/'\n",
    "\n",
    "    if Path(src).is_file() or Path(dst).is_file():\n",
    "        print('Incorrect paths, there must be a folders.')\n",
    "        sys.exit(-1)\n",
    "\n",
    "    # images in src - all in one where filenames start with label (category)\n",
    "    categories = identify_categories(src)\n",
    "    create_dataset_directories(dst, categories)\n",
    "\n",
    "    # if same extensions of images:\n",
    "    ext = {0:el.suffix[1:] for el in Path(src).iterdir() if el.is_file()}.get(0, '')\n",
    "\n",
    "    for category in categories:   # ! // \\\\ linux windows\n",
    "        copy_data(src, f'{dst}/train/{category}/', category, ext, 0, train_size)\n",
    "        copy_data(src, f'{dst}/validation/{category}/', category, ext, train_size, train_size + validation_size)\n",
    "        copy_data(src, f'{dst}/test/{category}/', category, ext, train_size + validation_size, train_size + validation_size + test_size)\n",
    "\n",
    "    return categories\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # main(sys.argv)\n",
    "\n",
    "    categories = main()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### __ImageDataGenerator__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "target_size = (150, 150)\n",
    "batch_size = 20\n",
    "# one of 'binary', 'categorical', 'input', 'multi_output', 'raw', 'sparse' or None. Default: 'categorical':\n",
    "class_mode = 'binary' if len(categories) == 2 else 'categorical'\n",
    "\n",
    "train_dir = '/media/denys/ftb/sprojects/datas/helps/data/train/'\n",
    "validation_dir = '/media/denys/ftb/sprojects/datas/helps/data/validation/'\n",
    "test_dir = '/media/denys/ftb/sprojects/datas/helps/data/test/'\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "                                                    train_dir,\n",
    "                                                    target_size=target_size,\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    class_mode=class_mode\n",
    "                                                    )\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "                                                        validation_dir,\n",
    "                                                        target_size=target_size,\n",
    "                                                        batch_size=batch_size,  # 1 !\n",
    "                                                        class_mode=class_mode\n",
    "                                                        )\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "                                                  test_dir,\n",
    "                                                  target_size=target_size,\n",
    "                                                  batch_size=1,\n",
    "                                                  class_mode=class_mode\n",
    "                                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<keras.src.preprocessing.image.DirectoryIterator at 0x7f63706abc10>,\n",
       " <keras.src.preprocessing.image.DirectoryIterator at 0x7f63706abb80>,\n",
       " <keras.src.preprocessing.image.DirectoryIterator at 0x7f6370766290>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator, validation_generator, test_generator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "generator = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "                                                                   train_dir,\n",
    "                                                                   target_size=target_size,\n",
    "                                                                   batch_size=1,\n",
    "                                                                   class_mode=class_mode\n",
    "                                                                   )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://pandas.pydata.org/docs/user_guide/merging.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.]\n",
      "[0.]\n",
      "[1.]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>67491</th>\n",
       "      <th>67492</th>\n",
       "      <th>67493</th>\n",
       "      <th>67494</th>\n",
       "      <th>67495</th>\n",
       "      <th>67496</th>\n",
       "      <th>67497</th>\n",
       "      <th>67498</th>\n",
       "      <th>67499</th>\n",
       "      <th>67500</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.784314</td>\n",
       "      <td>0.784314</td>\n",
       "      <td>0.737255</td>\n",
       "      <td>0.784314</td>\n",
       "      <td>0.776471</td>\n",
       "      <td>0.729412</td>\n",
       "      <td>0.780392</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.721569</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031373</td>\n",
       "      <td>0.027451</td>\n",
       "      <td>0.035294</td>\n",
       "      <td>0.031373</td>\n",
       "      <td>0.027451</td>\n",
       "      <td>0.035294</td>\n",
       "      <td>0.031373</td>\n",
       "      <td>0.027451</td>\n",
       "      <td>0.035294</td>\n",
       "      <td>0.031373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.384314</td>\n",
       "      <td>0.372549</td>\n",
       "      <td>0.305882</td>\n",
       "      <td>0.368627</td>\n",
       "      <td>0.356863</td>\n",
       "      <td>0.290196</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.321569</td>\n",
       "      <td>0.254902</td>\n",
       "      <td>...</td>\n",
       "      <td>0.807843</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.815686</td>\n",
       "      <td>0.890196</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.894118</td>\n",
       "      <td>0.945098</td>\n",
       "      <td>0.937255</td>\n",
       "      <td>0.949020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[1.0]</td>\n",
       "      <td>0.858824</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.847059</td>\n",
       "      <td>0.870588</td>\n",
       "      <td>0.847059</td>\n",
       "      <td>0.854902</td>\n",
       "      <td>0.870588</td>\n",
       "      <td>0.847059</td>\n",
       "      <td>0.847059</td>\n",
       "      <td>...</td>\n",
       "      <td>0.407843</td>\n",
       "      <td>0.482353</td>\n",
       "      <td>0.490196</td>\n",
       "      <td>0.403922</td>\n",
       "      <td>0.482353</td>\n",
       "      <td>0.490196</td>\n",
       "      <td>0.403922</td>\n",
       "      <td>0.478431</td>\n",
       "      <td>0.486275</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 67501 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0         1         2         3         4         5         6      \\\n",
       "0  [0.0]  0.784314  0.784314  0.737255  0.784314  0.776471  0.729412   \n",
       "1  [0.0]  0.384314  0.372549  0.305882  0.368627  0.356863  0.290196   \n",
       "2  [1.0]  0.858824  0.843137  0.847059  0.870588  0.847059  0.854902   \n",
       "\n",
       "      7         8         9      ...     67491     67492     67493     67494  \\\n",
       "0  0.780392  0.764706  0.721569  ...  0.031373  0.027451  0.035294  0.031373   \n",
       "1  0.333333  0.321569  0.254902  ...  0.807843  0.823529  0.823529  0.815686   \n",
       "2  0.870588  0.847059  0.847059  ...  0.407843  0.482353  0.490196  0.403922   \n",
       "\n",
       "      67495     67496     67497     67498     67499     67500  \n",
       "0  0.027451  0.035294  0.031373  0.027451  0.035294  0.031373  \n",
       "1  0.890196  0.882353  0.894118  0.945098  0.937255  0.949020  \n",
       "2  0.482353  0.490196  0.403922  0.478431  0.486275  0.400000  \n",
       "\n",
       "[3 rows x 67501 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = pd.DataFrame()\n",
    "for i, (images, y_batch) in enumerate(generator):\n",
    "    new_line = pd.DataFrame([[y_batch] + [images[0][a][b][c] for a in range(images[0].shape[0]) for b in range(images[0].shape[1]) for c in range(images[0].shape[2])]], index=[i])\n",
    "    data_df = pd.concat([data_df, new_line])\n",
    "    if i == 2:\n",
    "        break\n",
    "       \n",
    "# pd.Dataframe.to_csv('data.csv', index=False)\n",
    "data_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.to_csv('/media/denys/ftb/sprojects/datas/helps/data/data.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading `from files:`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "\n",
    "def read_from_csvfile(file: Path, header: Optional[str]='infer') -> pd.DataFrame:\n",
    "    \"\"\"Read content from csv-file and return dataframe from content.\"\"\"\n",
    "    df = pd.read_csv(file, header=header)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0         1         2         3         4         5         6         7  \\\n",
      "0  [0.]  0.952941  0.949020  0.831373  0.964706  0.960784  0.850980  0.956863   \n",
      "1  [1.]  0.352941  0.321569  0.313725  0.352941  0.321569  0.313725  0.356863   \n",
      "2  [0.]  0.882353  0.850980  0.807843  0.886275  0.854902  0.811765  0.909804   \n",
      "\n",
      "          8         9  ...     67491     67492     67493     67494     67495  \\\n",
      "0  0.949020  0.850980  ...  0.396078  0.800000  0.666667  0.388235  0.788235   \n",
      "1  0.325490  0.317647  ...  0.203922  0.152941  0.184314  0.196078  0.113725   \n",
      "2  0.870588  0.835294  ...  0.403922  0.207843  0.313725  0.396078  0.231373   \n",
      "\n",
      "      67496     67497     67498     67499     67500  \n",
      "0  0.654902  0.376471  0.772549  0.639216  0.360784  \n",
      "1  0.145098  0.156863  0.164706  0.196078  0.207843  \n",
      "2  0.333333  0.423529  0.223529  0.317647  0.419608  \n",
      "\n",
      "[3 rows x 67501 columns]       0         1         2         3         4         5         6         7  \\\n",
      "0  [0.]  0.952941  0.949020  0.831373  0.964706  0.960784  0.850980  0.956863   \n",
      "1  [1.]  0.352941  0.321569  0.313725  0.352941  0.321569  0.313725  0.356863   \n",
      "2  [0.]  0.882353  0.850980  0.807843  0.886275  0.854902  0.811765  0.909804   \n",
      "\n",
      "          8         9  ...     67491     67492     67493     67494     67495  \\\n",
      "0  0.949020  0.850980  ...  0.396078  0.800000  0.666667  0.388235  0.788235   \n",
      "1  0.325490  0.317647  ...  0.203922  0.152941  0.184314  0.196078  0.113725   \n",
      "2  0.870588  0.835294  ...  0.403922  0.207843  0.313725  0.396078  0.231373   \n",
      "\n",
      "      67496     67497     67498     67499     67500  \n",
      "0  0.654902  0.376471  0.772549  0.639216  0.360784  \n",
      "1  0.145098  0.156863  0.164706  0.196078  0.207843  \n",
      "2  0.333333  0.423529  0.223529  0.317647  0.419608  \n",
      "\n",
      "[3 rows x 67501 columns]\n"
     ]
    }
   ],
   "source": [
    "# read_from_csvfile(item, None)\n",
    "\n",
    "# 'https://drive.google.com/u/0/uc?id=1JMYqXipZpz9Y5-vyxvLEO2Y1sRBxqu-U&export=download'\n",
    "# data = pd.read_csv('/media/denys/ftb/sprojects/datas/helps/data/sampleSubmission.csv')\n",
    "data = pd.read_csv('/media/denys/ftb/sprojects/datas/helps/data/data.csv')\n",
    "print(data.head(3), data.tail(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>67491</th>\n",
       "      <th>67492</th>\n",
       "      <th>67493</th>\n",
       "      <th>67494</th>\n",
       "      <th>67495</th>\n",
       "      <th>67496</th>\n",
       "      <th>67497</th>\n",
       "      <th>67498</th>\n",
       "      <th>67499</th>\n",
       "      <th>67500</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.952941</td>\n",
       "      <td>0.949020</td>\n",
       "      <td>0.831373</td>\n",
       "      <td>0.964706</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.850980</td>\n",
       "      <td>0.956863</td>\n",
       "      <td>0.949020</td>\n",
       "      <td>0.850980</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>...</td>\n",
       "      <td>0.396078</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.388235</td>\n",
       "      <td>0.788235</td>\n",
       "      <td>0.654902</td>\n",
       "      <td>0.376471</td>\n",
       "      <td>0.772549</td>\n",
       "      <td>0.639216</td>\n",
       "      <td>0.360784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.321569</td>\n",
       "      <td>0.313725</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.321569</td>\n",
       "      <td>0.313725</td>\n",
       "      <td>0.356863</td>\n",
       "      <td>0.325490</td>\n",
       "      <td>0.317647</td>\n",
       "      <td>0.356863</td>\n",
       "      <td>...</td>\n",
       "      <td>0.203922</td>\n",
       "      <td>0.152941</td>\n",
       "      <td>0.184314</td>\n",
       "      <td>0.196078</td>\n",
       "      <td>0.113725</td>\n",
       "      <td>0.145098</td>\n",
       "      <td>0.156863</td>\n",
       "      <td>0.164706</td>\n",
       "      <td>0.196078</td>\n",
       "      <td>0.207843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.850980</td>\n",
       "      <td>0.807843</td>\n",
       "      <td>0.886275</td>\n",
       "      <td>0.854902</td>\n",
       "      <td>0.811765</td>\n",
       "      <td>0.909804</td>\n",
       "      <td>0.870588</td>\n",
       "      <td>0.835294</td>\n",
       "      <td>0.917647</td>\n",
       "      <td>...</td>\n",
       "      <td>0.403922</td>\n",
       "      <td>0.207843</td>\n",
       "      <td>0.313725</td>\n",
       "      <td>0.396078</td>\n",
       "      <td>0.231373</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.423529</td>\n",
       "      <td>0.223529</td>\n",
       "      <td>0.317647</td>\n",
       "      <td>0.419608</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 67500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          1         2         3         4         5         6         7  \\\n",
       "0  0.952941  0.949020  0.831373  0.964706  0.960784  0.850980  0.956863   \n",
       "1  0.352941  0.321569  0.313725  0.352941  0.321569  0.313725  0.356863   \n",
       "2  0.882353  0.850980  0.807843  0.886275  0.854902  0.811765  0.909804   \n",
       "\n",
       "          8         9        10  ...     67491     67492     67493     67494  \\\n",
       "0  0.949020  0.850980  0.960784  ...  0.396078  0.800000  0.666667  0.388235   \n",
       "1  0.325490  0.317647  0.356863  ...  0.203922  0.152941  0.184314  0.196078   \n",
       "2  0.870588  0.835294  0.917647  ...  0.403922  0.207843  0.313725  0.396078   \n",
       "\n",
       "      67495     67496     67497     67498     67499     67500  \n",
       "0  0.788235  0.654902  0.376471  0.772549  0.639216  0.360784  \n",
       "1  0.113725  0.145098  0.156863  0.164706  0.196078  0.207843  \n",
       "2  0.231373  0.333333  0.423529  0.223529  0.317647  0.419608  \n",
       "\n",
       "[3 rows x 67500 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = data['0'].to_numpy()\n",
    "# за замовчуванням axis=0, що означає роботу з рядками. Якщо вказати axis=1, то це дозволить видаляти стовпці:\n",
    "data.drop(['0'], inplace=True, axis=1) \n",
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading `from Link:`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[               Регіон   1950   1960   1970  1990  2000  2012 2014 2019\n",
       " 0                Крим  230.0  206.0  160.0   130    73   126    —    —\n",
       " 1           Вінницька  224.0  192.0  142.0   124    84   112  109   76\n",
       " 2           Волинська  247.0  250.0  179.0   153   112   148  141  101\n",
       " 3    Дніпропетровська  204.0  204.0  151.0   123    71   112  111   71\n",
       " 4            Донецька  271.0  214.0  140.0   109    61    98   82    —\n",
       " 5         Житомирська  261.0  223.0  159.0   129    89   122  120   79\n",
       " 6        Закарпатська  314.0  273.0  207.0   168   115   151  146  104\n",
       " 7          Запорізька  219.0  197.0  150.0   124    71   106  106   68\n",
       " 8   Івано-Франківська  243.0  248.0  182.0   155   103   124  122   88\n",
       " 9            Київська  204.0  189.0  156.0   123    73   122  121   80\n",
       " 10     Кіровоградська  216.0  171.0  145.0   126    79   110  108   68\n",
       " 11          Луганська  262.0  235.0  144.0   116    62    96   51    —\n",
       " 12          Львівська  234.0  240.0  171.0   140    91   119  119   87\n",
       " 13       Миколаївська  211.0  194.0  155.0   137    80   115  112   71\n",
       " 14            Одеська  241.0  192.0  148.0   126    80   127  123   88\n",
       " 15         Полтавська  186.0  163.0  131.0   118    70    99  100   65\n",
       " 16         Рівненська  269.0  267.0  193.0   158   118   159  148  107\n",
       " 17            Сумська  216.0  184.0  132.0   115    70    97   92   60\n",
       " 18      Тернопільська  213.0  216.0  157.0   142    92   113  109   76\n",
       " 19         Харківська  197.0  173.0  140.0   114    68    99  101   68\n",
       " 20         Херсонська  208.0  214.0  166.0   143    85   117  115   81\n",
       " 21        Хмельницька  234.0  198.0  148.0   129    85   113  112   79\n",
       " 22          Черкаська  205.0  179.0  144.0   123    75   100   98   64\n",
       " 23        Чернівецька  247.0  218.0  170.0   148   101   128  129   92\n",
       " 24       Чернігівська  220.0  183.0  127.0   108    69    94   90   61\n",
       " 25               Київ    NaN  174.0  159.0   120    73   120  121  110\n",
       " 26        Севастополь    NaN    NaN    NaN   125    70   120    —    —\n",
       " 27            Україна  228.0  205.0  152.0   126    78   114  111   81]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1t1 = pd.read_html(\n",
    "                    io='https://uk.wikipedia.org/wiki/%D0%9D%D0%B0%D1%81%D0%B5%D0%BB%D0%B5%D0%BD%D0%BD%D1%8F_%D0%A3%D0%BA%D1%80%D0%B0%D1%97%D0%BD%D0%B8',\n",
    "                    match='Коефіцієнт народжуваності в регіонах',\n",
    "                    )\n",
    "p1t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __`MODEL`__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping\n",
    "- https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_early_stopping = EarlyStopping(\n",
    "                                        monitor='val_categorical_accuracy',  # val_loss\n",
    "                                        patience=4,\n",
    "                                        )\n",
    "\n",
    "callback_save = ModelCheckpoint(\n",
    "                                'best.hdf5',  # name to save the resulting model\n",
    "                                monitor='val_loss',\n",
    "                                save_best_only=True,\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __`Training`__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "                    train_dataset,\n",
    "                    validation_data=valid_dataset,\n",
    "                    epochs=20,\n",
    "                    verbose=1, # print logs\n",
    "                    callbacks=[callback_early_stopping, callback_save]  # callback_save\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "                    train_datagen,\n",
    "                    steps_per_epoch=100,\n",
    "                    epochs=100,\n",
    "                    validation_data=valid_datagen,\n",
    "                    validation_steps=100\n",
    "                    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://stackoverflow.com/questions/62836066/infinite-loop-with-imagedatagenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "                    train_generator,\n",
    "                    steps_per_epoch=100,\n",
    "                    epochs=30,\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps=50\n",
    "                    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __`INTERPRET`__ / __`Error Analysis`__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __`RE-TEST`__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __`RESULTS` & `CONCLUSIONS`__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datas-cYPLqW4U-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
